{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx3HOZpLean1"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpU1PuV65-kn"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TIgMT5P6G8j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import datetime\n",
    "from numpy import array, linspace\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.pyplot import plot\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTSsz2_x6NQh"
   },
   "outputs": [],
   "source": [
    "from astral.sun import sun\n",
    "from astral import LocationInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAoFJ75C6ulo"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90t3SOX26SB3"
   },
   "outputs": [],
   "source": [
    "def timeofday(x):\n",
    "    if (x == 23) or (x <5):\n",
    "        return 'night'\n",
    "    elif (x>=5) and (x<9):\n",
    "        return 'early_morning'\n",
    "    elif (x>=9) and (x<15):\n",
    "        return 'morning'\n",
    "    elif (x>=15) and (x<20):\n",
    "        return 'afternoon'\n",
    "    elif (x>=20) and (x<23):\n",
    "        return 'evening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecb955NW6V3p"
   },
   "outputs": [],
   "source": [
    "def X_prep(data):\n",
    "    \n",
    "    data = data.drop(columns = [data.columns[-1]])\n",
    "    \n",
    "    data[\"time_step\"] = pd.to_datetime(data[\"time_step\"], format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    \n",
    "    # half_hour\n",
    "    data[\"half_h\"] = data['time_step'].dt.floor('30T').dt.time\n",
    "    data[\"half_hour\"] = data[\"half_h\"].apply(lambda x: str(x)[:5])\n",
    "    data = data.drop('half_h', axis = 1)\n",
    "\n",
    "    # hour, weekday, week, month\n",
    "    data['hour'] = data.time_step.dt.hour\n",
    "    data['weekday'] = data.time_step.dt.weekday\n",
    "    data['week'] = data.time_step.dt.week\n",
    "    data['month'] = data.time_step.dt.month\n",
    "    \n",
    "    # season\n",
    "    seasons = ['winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', \n",
    "               'fall', 'fall', 'fall', 'winter']\n",
    "    month_to_season = dict(zip(range(1,13), seasons))\n",
    "    data['season'] = data.month.map(month_to_season)\n",
    "\n",
    "    # timeofday\n",
    "    data['timeofday'] = data.hour.apply(lambda x: timeofday(x))\n",
    "    \n",
    "    # daylight time inversed\n",
    "    city = LocationInfo(\"Paris\")\n",
    "    _ = data.time_step.apply(lambda x: sun(city.observer, date=x.date()))\n",
    "    data[\"daylight_inv\"] = _.apply(lambda x: 1/((x[\"sunset\"] - x[\"sunrise\"]).seconds/3600))\n",
    "    \n",
    "    # dealing with Consumption missing values\n",
    "    data['consumption'].interpolate(method='linear', inplace=True)\n",
    "    \n",
    "    # dealing with weather missing values\n",
    "    data[\"datetime\"] = data[\"time_step\"].apply(lambda x: \"{:%m, %d, %H}\".format(x))\n",
    "    data.iloc[0,2:9] = data.iloc[59,2:9]\n",
    "    _ = data[[\"datetime\",\"visibility\",\"temperature\",\"humidity\",\"humidex\",\"windchill\",\"wind\",\"pressure\"]] \n",
    "    info = _.drop_duplicates(subset =\"datetime\", keep = \"first\").reset_index(drop=True)\n",
    "    _ = info.drop(columns = ['datetime'])\n",
    "    _.interpolate(method='linear', inplace=True)\n",
    "    info.iloc[:,1:] = _\n",
    "    data.iloc[:,2:9] = pd.merge(info,data.datetime, on=\"datetime\").iloc[:,1:8]\n",
    "  \n",
    "    # drop time_step, datetime\n",
    "    data = data.drop([\"datetime\"], axis = 1)\n",
    "    data.set_index(\"time_step\", inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vX_s6Bz6tRc"
   },
   "outputs": [],
   "source": [
    "X_train = Xtrain.copy()\n",
    "X_train = X_prep(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68K3tUQT9Ef3"
   },
   "outputs": [],
   "source": [
    "def y_prep(y_train):\n",
    "    y_train.time_step = pd.to_datetime(y_train.time_step)\n",
    "    y_train.set_index(\"time_step\", inplace=True)\n",
    "    y_train.interpolate(method='linear', inplace=True)\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xk3861pdgInF"
   },
   "outputs": [],
   "source": [
    "y_train = ytrain.copy()\n",
    "y_train = y_prep(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VbWkfDwwRnI"
   },
   "source": [
    "# Data Exploration\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xu-b5dOr8oUw"
   },
   "outputs": [],
   "source": [
    "data_explor = pd.concat([X_train,\n",
    "                         y_train[['washing_machine','fridge_freezer','TV','kettle']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xQZOu15QPfW"
   },
   "source": [
    "### General exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TeibMqQ7_Iv"
   },
   "source": [
    "Before looking at our data_explor dataframe, let's check the state of our target variables before the transformations we applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "URZhbJ9s8ViP",
    "outputId": "78f36f59-1daa-4830-c3d7-d0b1b3c0a493"
   },
   "outputs": [],
   "source": [
    "ytrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wlm_NeO18dha",
    "outputId": "3adee72b-00bd-4328-8a47-13e5ecabc5fa"
   },
   "outputs": [],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "A9RiPEGd8JyA",
    "outputId": "834cc94a-2b36-48b3-eb59-29931685fc96"
   },
   "outputs": [],
   "source": [
    "ytrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okCgK8ET8cGb"
   },
   "source": [
    "We are dealing with 4 target variables and we have 417,599 observations in our training dataset. Among these observations, 10,231 values are null (or 2.4% of the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2iRQVHN5uph"
   },
   "source": [
    "After having applied a linear interpolation to fill the null values, here are some basic information on the target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "hNWtfZ225xuU",
    "outputId": "7ff1416d-86e6-47ba-8c85-573c6ca856a7"
   },
   "outputs": [],
   "source": [
    "data_explor[[\"washing_machine\", \"fridge_freezer\", \"TV\", \"kettle\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zg6G4Pvh6E5A"
   },
   "source": [
    "- **Washing Machine**: the standard deviation is 10 times the mean, which warns us about the repartition of our data. Furthermore, the 3rd quartile is 0, meaning that at least 75% of the data is zero. Moreover, the maximum value is 486 the mean value. We may have extreme values.\n",
    "\n",
    "- **Fridge Freezer**: Here, the standard deviation is around the same value as the mean. At least 25% of the data is zero, which is sparse.\n",
    "\n",
    "- **TV**: Regarding TV, all the values seem to stay around 7. The repartition looks more uniform.\n",
    "\n",
    "- **Kettle**: We seem to be in the same situation as that of Washing Machine. There are some extreme values and the data is very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go9UV2F63D-o"
   },
   "source": [
    "Let us first explore the repartition of our target variables with histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "0_9zdWgk3K-u",
    "outputId": "35614a5b-95bc-49f1-c49f-81707e7d7add"
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "fig, ax = plt.subplots(1,4, figsize=(18,2))\n",
    "ax[0].hist(data_explor.washing_machine)\n",
    "ax[0].set_title(\"Washing machine\")\n",
    "\n",
    "ax[1].hist(data_explor.fridge_freezer)\n",
    "ax[1].set_title(\"Fridge Freezer\")\n",
    "\n",
    "ax[2].hist(data_explor.TV)\n",
    "ax[2].set_title(\"TV\")\n",
    "\n",
    "ax[3].hist(data_explor.kettle)\n",
    "ax[3].set_title(\"Kettle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9lqGVSO4Epv"
   },
   "source": [
    "In any case, we are dealing with very sparse variables and none of them is normally distributed. \n",
    "\n",
    "Washing Machine and Kettle seem to have very few and extremely large values. Let us observe the repartition of the extreme values for these two variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "dBPoAX2i40zN",
    "outputId": "c18ac45b-a8c2-4ff8-cafc-e20a6676fedd"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,2))\n",
    "ax[0].hist(data_explor.washing_machine[data_explor.washing_machine > 100])\n",
    "ax[0].set_title(\"Washing machine\")\n",
    "\n",
    "ax[1].hist(data_explor.kettle[data_explor.kettle > 100])\n",
    "ax[1].set_title(\"Kettle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xb0fKB6Q3Lix"
   },
   "source": [
    "Now, what about the correlations between all available variables ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "gOg_zJp2QOw4",
    "outputId": "76fdfab6-167e-4685-df8b-831c6833d81b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cor = data_explor.corr()\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cijdk8T_Q3KA"
   },
   "source": [
    "- The variables seem fairly correlated to Consumption, especially Kettle\n",
    "- TV is correlated to the hour of the day\n",
    "- The environmental informations (humidity, temperature...) are quite correlated \n",
    "- However, there is low correlation between our target variables and the environmental information\n",
    "- Our target variables are not correlated with one another, ruling out the possibility of using the predictions of one variable to predict another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVAsyylhwSnc"
   },
   "source": [
    "Let us observe in more details the relation between consumption and our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "Jrpbjo9PwY7U",
    "outputId": "769800a1-9d2c-4162-a024-353fd4d41122"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(18,4))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = np.logical_and(data_explor.index.month == 5, data_explor.index.day==11 + i)\n",
    "    ax[i].plot(data_explor[mask].washing_machine, label=\"washing_machine\")\n",
    "    ax[i].plot(data_explor[mask].consumption, label=\"consumption\")\n",
    "    ax[i].set_title(\"Month 5, day \" + str(11+i))\n",
    "    ax[i].legend()\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTehmGng0pAq"
   },
   "source": [
    "We can notice a specific pattern in consumption when the washing machine is in use. We first have a large increase and then many ups and downs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "tiYHhkY603Cj",
    "outputId": "cf883cc6-8a9d-4b27-9910-ffe3f8aba852"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(18,4))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = np.logical_and(data_explor.index.month == 5, data_explor.index.day==11 + i)\n",
    "    ax[i].plot(data_explor[mask].fridge_freezer, label=\"fridge_freezer\")\n",
    "    ax[i].plot(data_explor[mask].consumption, label=\"consumption\")\n",
    "    ax[i].set_title(\"Month 5, day \" + str(11+i))\n",
    "    ax[i].legend()\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaVbfg9n1TLi"
   },
   "source": [
    "It seems that consumption reproduces the behavior of that of the fridge_freezer, but not always. That's why there is also a small correlation between the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "pFa1aPnc2HHB",
    "outputId": "b1bcdadf-adcc-4ce4-8d3b-1e5cf75174c7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(18,4))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = np.logical_and(data_explor.index.month == 5, data_explor.index.day==11 + i)\n",
    "    ax[i].plot(data_explor[mask].TV, label=\"TV\")\n",
    "    ax[i].plot(data_explor[mask].consumption, label=\"consumption\")\n",
    "    ax[i].set_title(\"Month 5, day \" + str(11+i))\n",
    "    ax[i].legend()\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWU9zmHU2QPs"
   },
   "source": [
    "TV does not seem to influence on the overall consumption at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "llCrjs1Y2YUt",
    "outputId": "e79a8ed2-6021-49fa-f30a-5dee663fbe5e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(18,4))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = np.logical_and(data_explor.index.month == 4, data_explor.index.day==11 + i)\n",
    "    ax[i].plot(data_explor[mask].kettle, label=\"Kettle\")\n",
    "    ax[i].plot(data_explor[mask].consumption, label=\"consumption\")\n",
    "    ax[i].set_title(\"Month 5, day \" + str(11+i))\n",
    "    ax[i].legend()\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGpRlsiH2oPs"
   },
   "source": [
    "Whenever the kettle is turned on, we observe a peak in the overall consumption. It's a very sharp peak which seems to last only for a few minutes. It could be very interesting to spot that peak in our data in order to find the kettle consumption peak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0sVY5ru8CRG"
   },
   "source": [
    "### By month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "jFh3iqzTO1Y3",
    "outputId": "a5a295d3-fefb-4b7f-f164-466bc4c64595"
   },
   "outputs": [],
   "source": [
    "by_month = data_explor[['consumption','washing_machine','fridge_freezer',\n",
    "                        'TV','kettle','month']].groupby('month').mean()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,2))\n",
    "\n",
    "by_month.washing_machine.plot.bar(ax=axes[0],title='Washing Machine')\n",
    "by_month.fridge_freezer.plot.bar(ax=axes[1],title='Fridge Freezer')\n",
    "by_month.TV.plot.bar(ax=axes[2],title='TV')\n",
    "by_month.kettle.plot.bar(ax=axes[3],title='Kettle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0vslXPUaQSE"
   },
   "source": [
    "We can make the following observations:\n",
    "* **Kettle:** it is more used during fall and winter (starting October) than during warmer months\n",
    "* It is difficult to observe a significant correlation between months and the consumption of fridge, washing machine and TV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJiFzB9jvCGw"
   },
   "source": [
    "### By season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "uyfRFNn9SxVX",
    "outputId": "596f262f-4be0-4c15-ca3d-a33df6b288d0"
   },
   "outputs": [],
   "source": [
    "by_hour = data_explor[['consumption','washing_machine','fridge_freezer',\n",
    "                       'TV','kettle','hour','season']]\n",
    "by_hour_spring = by_hour[by_hour.season == \"spring\"].groupby(['hour']).mean()\n",
    "by_hour_summer = by_hour[by_hour.season == \"summer\"].groupby(['hour']).mean()\n",
    "by_hour_winter = by_hour[by_hour.season == \"winter\"].groupby(['hour']).mean()\n",
    "by_hour_fall = by_hour[by_hour.season == \"fall\"].groupby(['hour']).mean()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18,4))\n",
    "\n",
    "by_hour_spring.washing_machine.plot(ax=axes[0],title='Washing Machine')\n",
    "by_hour_summer.washing_machine.plot(ax=axes[0])\n",
    "by_hour_winter.washing_machine.plot(ax=axes[0])\n",
    "by_hour_fall.washing_machine.plot(ax=axes[0])\n",
    "\n",
    "by_hour_spring.fridge_freezer.plot(ax=axes[1],title='Fridge Freezer')\n",
    "by_hour_summer.fridge_freezer.plot(ax=axes[1])\n",
    "by_hour_winter.fridge_freezer.plot(ax=axes[1])\n",
    "by_hour_fall.fridge_freezer.plot(ax=axes[1])\n",
    "\n",
    "by_hour_spring.TV.plot(ax=axes[2],title='TV')\n",
    "by_hour_summer.TV.plot(ax=axes[2])\n",
    "by_hour_winter.TV.plot(ax=axes[2])\n",
    "by_hour_fall.TV.plot(ax=axes[2])\n",
    "\n",
    "by_hour_spring.kettle.plot(ax=axes[3],title='Kettle')\n",
    "by_hour_summer.kettle.plot(ax=axes[3])\n",
    "by_hour_winter.kettle.plot(ax=axes[3])\n",
    "by_hour_fall.kettle.plot(ax=axes[3])\n",
    "\n",
    "axes[0].legend(['spring','summer','winter','fall'])\n",
    "axes[1].legend(['spring','summer','winter','fall'])\n",
    "axes[2].legend(['spring','summer','winter','fall'])\n",
    "axes[3].legend(['spring','summer','winter','fall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ru6yQZozezxd"
   },
   "source": [
    "- **Fridge_freezer**: consumption seems to be higher in summer and fall than in spring and winter\n",
    "- **TV**: consumption is pretty homogenous among season. TV is more used in the morning and early afternoon during spring. During winter it is used later at night.\n",
    "- **Kettle**: consumption is significantly higher during winter and fall\n",
    "- **Washing machine**: consumption seems homogenous among seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IaR_GUj2Rb-z"
   },
   "source": [
    "### By weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "-P6fGltYPgRj",
    "outputId": "6379ee99-6509-4960-b7e5-3e1740fc9087"
   },
   "outputs": [],
   "source": [
    "by_weekday = data_explor[['consumption','washing_machine','fridge_freezer',\n",
    "                          'TV','kettle','weekday']].groupby('weekday').mean()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,2))\n",
    "\n",
    "by_weekday.washing_machine.plot.bar(ax=axes[0],title='Washing Machine')\n",
    "by_weekday.fridge_freezer.plot.bar(ax=axes[1],title='Fridge Freezer')\n",
    "by_weekday.TV.plot.bar(ax=axes[2],title='TV')\n",
    "by_weekday.kettle.plot.bar(ax=axes[3],title='Kettle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndaQh00LgBek"
   },
   "source": [
    "- Weekday doesn't seem to have an importance for the consumption of fridge, TV and Kettle\n",
    "- Washing machine is more used during the weekend, notably on Sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDS6o0jWu6Gd"
   },
   "source": [
    "### By time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "ebLIUQPcTt08",
    "outputId": "bed3f44e-e967-438e-f4ce-de3804c375c7"
   },
   "outputs": [],
   "source": [
    "by_timeofday = data_explor[['consumption','washing_machine','fridge_freezer',\n",
    "                            'TV','kettle','timeofday']].groupby('timeofday').mean()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,2))\n",
    "\n",
    "by_timeofday.washing_machine.plot.bar(ax=axes[0],title='Washing Machine')\n",
    "by_timeofday.fridge_freezer.plot.bar(ax=axes[1],title='Fridge Freezer')\n",
    "by_timeofday.TV.plot.bar(ax=axes[2],title='TV')\n",
    "by_timeofday.kettle.plot.bar(ax=axes[3],title='Kettle')                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srP5_LWchEBH"
   },
   "source": [
    "This is how we defined the times of day: 5am-9am is **early_morning**, 9am-3pm is **morning**, 3pm-8pm **afternoon**, 8pm-11pm is **evening**, 11pm-5am is **night**\n",
    "- **Fridge**: consumption is homogeneous during the day\n",
    "- **TV**: consumption is higher during afternoon and evening\n",
    "- **Kettle**: it consumes the most in the afternoon\n",
    "- **Washing machine**: consumption is higher during evening and night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XsMaG4-5Oeb"
   },
   "source": [
    "# Final Models\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYU3eBv6r8j8"
   },
   "source": [
    "## Fridge Freezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vmVmyrzuCBj"
   },
   "source": [
    "**Pattern** \\\n",
    "Thanks to the Exploratory Data Analysis, we are able to notice that the pattern in the fridge_freezer consumption is often reflected on the pattern of the overall consumption. However, if the overall consumption is above a certain level, the pattern of the fridge freezer consumption is no longer reflected on the overall consumption. \n",
    "\n",
    "Therefore, we put in our training data the overall consumption over the past 30 minutes to caracterize the pattern; however, if the consumption is higher than 600, we leave that value at 600: it is capped. \n",
    "\n",
    "**Weather Information** \\\n",
    "It seems that this information influence the fridge_freezer consumption, so we keep it. This makes sense since, for example, the higher the outdoor temperature, the higher the indoor temperature and so the more energy the fridge freezer needs to keep its inside temperature low. \n",
    "\n",
    "**Temporal Information** \\\n",
    "Week, weekday and hour of the day are taken into account. \n",
    "\n",
    "**Model** \\\n",
    "We use a boosting method: the Light GBM algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q99W7T-er_K2"
   },
   "outputs": [],
   "source": [
    "class fridge_freezer():\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMRegressor(n_estimators=1700, min_child_weight=1.5, max_depth=12, \n",
    "                               max_bin=100, colsample_bytree=0.6)\n",
    "    \n",
    "    \n",
    "    def shift_data(self, data, lag=30):\n",
    "        series = [data.shift(i) for i in range(lag-1, -1, -1)]\n",
    "        new_df = pd.concat(series, axis=1)\n",
    "        name_col = []\n",
    "        \n",
    "        for i in range(lag, 0, -1):\n",
    "            var = \"consumption -\" + str(i)\n",
    "            name_col.append(var)\n",
    "        \n",
    "        new_df.columns = name_col\n",
    "        new_df[\"std\"] = new_df.std(axis=1)\n",
    "        new_df[\"mean\"] = new_df.mean(axis=1)\n",
    "        return new_df\n",
    "    \n",
    "    def transform(self, X_train):\n",
    "        X_train = X_train.drop(columns=[\"Unnamed: 9\"])\n",
    "        X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "        X_train[\"week\"] = X_train[\"time_step\"].map(lambda x: x.week)\n",
    "        X_train[\"weekday\"] = X_train[\"time_step\"].map(lambda x: x.weekday)\n",
    "        X_train[\"hour\"] = X_train[\"time_step\"].map(lambda x: x.hour)\n",
    "        \n",
    "        #Missing values\n",
    "        X_train.fillna(method=\"ffill\", inplace=True) #Get the previous non null values\n",
    "        \n",
    "        #Conditions on consumption\n",
    "        X_train[\"capped_consumption\"] = np.where(X_train.consumption>=600, 600, X_train.consumption)\n",
    "        \n",
    "        #Supervised consumption\n",
    "        supervised_consumption = self.shift_data(X_train.consumption)\n",
    "        X_train = X_train.join(supervised_consumption)\n",
    "        \n",
    "        #One hot encoding\n",
    "        X_train_hour = pd.Categorical(X_train.hour, categories = [i for i in range(24)])\n",
    "        X_train_weekday = pd.Categorical(X_train.weekday, categories = [i for i in range(7)])\n",
    "        X_train_week = pd.Categorical(X_train.week, categories = [i for i in range(52)])\n",
    "        \n",
    "        X_train = X_train.join(pd.get_dummies(X_train_hour, prefix=\"hour\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_weekday, prefix=\"weekday\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_week, prefix=\"week\"))\n",
    "        \n",
    "        X_train.drop(columns=[\"time_step\", \"hour\", \"weekday\", \"week\"], inplace=True)\n",
    "        return X_train\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred = np.where(y_pred<0, 0, y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g8S0yRYr_Gs"
   },
   "outputs": [],
   "source": [
    "prep = fridge_freezer()\n",
    "X_train = prep.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsanYQPUr-9r"
   },
   "outputs": [],
   "source": [
    "# Drop na\n",
    "data = X_train.join(ytrain)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-5]\n",
    "y_train = data.iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOQ0NrwMxDbO"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO-jwbkExFjV"
   },
   "outputs": [],
   "source": [
    "# Fitting of the model\n",
    "prep.fit(X_train, y_train.fridge_freezer)\n",
    "\n",
    "# Predict\n",
    "y_pred = prep.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-lrCEhI1xFbk",
    "outputId": "13e57e22-53e9-47df-ce3f-c830a23d00da"
   },
   "outputs": [],
   "source": [
    "print(\"R2 of\", r2_score(y_test.fridge_freezer, y_pred))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test.fridge_freezer, y_pred)))\n",
    "print(\"Adjusted RMSE:\", np.sqrt(mean_squared_error(y_test.fridge_freezer, y_pred)*49.79/74.86))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bhrgltsyp-S4"
   },
   "source": [
    "## Kettle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC3QeL_SqEnr"
   },
   "source": [
    "As seen in the exploratory data anaylsis, the overall consumption is very much correlated with the Kettle consumption. More specifically, we notice that, usually, when the overall consumption is below a certain level, the kettle is not turned on. \n",
    "\n",
    "Therefore, we decided to create **two models**: \n",
    "- One model when the overall consumption was below 600\n",
    "- Another model when the overall consumption was above 600 (and the values to predict were likely to go to extremes)\n",
    "\n",
    "**Type of models** \\\n",
    "Boosting was the chosen method for this regression problem. More specifically, we selected the XGBoost algorithm.\n",
    "\n",
    "**Peak** \\\n",
    "Since a peak in consumption is likely to equal a peak in the kettle consumption, we wanted to caracterize the peak in the overall consumption in our training data. To do so, we decided to take the overall consumption over the last 20 minutes, their mean and standard deviation. Thanks to this data we can say that, if the standard deviation is high and the mean is high, we are at the beginning of the peak: we should predict a high value for kettle. \n",
    "\n",
    "**Temporal Information** \\\n",
    "We chose to keep the following temporal features, as they seemed relevant in our exploratory data analysis: hour, weekday and week number. \n",
    "\n",
    "**Weather Information** \\\n",
    "We also dropped the environmental information (temperature, humidity...) as they didn't seem to have an impact on the kettle consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JT7FTNadp2lN"
   },
   "outputs": [],
   "source": [
    "class kettle():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_0 = xgb.XGBRegressor(min_child_weight=1.5, max_depth= 8, gamma=0.6, colsample_bytree=0.9)\n",
    "        self.model_1 = xgb.XGBRegressor(min_child_weight=1.5, max_depth= 8, gamma=0.6, colsample_bytree=0.9)\n",
    "        \n",
    "    def shift_data(self, data, lag=20):\n",
    "        series = [data.shift(i) for i in range(lag-1, -1, -1)]\n",
    "        new_df = pd.concat(series, axis=1)\n",
    "        name_col = []\n",
    "        \n",
    "        for i in range(lag, 0, -1):\n",
    "            var = \"consumption -\" + str(i)\n",
    "            name_col.append(var)\n",
    "        \n",
    "        new_df.columns = name_col\n",
    "        new_df[\"std\"] = new_df.std(axis=1)\n",
    "        new_df[\"mean\"] = new_df.mean(axis=1)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def transform(self, X_train):\n",
    "        X_train = X_train[[\"time_step\", \"consumption\"]]\n",
    "        X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "        \n",
    "        X_train[\"hour\"] = X_train[\"time_step\"].map(lambda x: x.hour)\n",
    "        X_train[\"weekday\"] = X_train[\"time_step\"].map(lambda x: x.dayofweek)\n",
    "        X_train[\"week\"] = X_train[\"time_step\"].map(lambda x: x.week)\n",
    "        \n",
    "        #More information on consumption\n",
    "        X_train[\"cluster\"] = np.where(X_train.consumption > 600, 1, 0)\n",
    "        \n",
    "        #Missing values\n",
    "        X_train.fillna(method=\"ffill\", inplace=True) #Get the previous non null values\n",
    "        \n",
    "        #Supervised consumption\n",
    "        supervised_consumption = self.shift_data(X_train.consumption)\n",
    "        \n",
    "        #One hot encoding\n",
    "        X_train_hour = pd.Categorical(X_train.hour, categories = [i for i in range(24)])\n",
    "        X_train_weekday = pd.Categorical(X_train.weekday, categories = [i for i in range(7)])\n",
    "        X_train_week = pd.Categorical(X_train.week, categories = [i for i in range(52)])\n",
    "\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_hour, prefix=\"hour\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_weekday, prefix=\"weekday\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_week, prefix=\"week\"))\n",
    "        \n",
    "        X_train.drop(columns=[\"time_step\",\"hour\", \"weekday\", \"week\"], inplace=True)\n",
    "        \n",
    "        return X_train.join(supervised_consumption)\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train_0 = X_train[X_train.cluster == 0]\n",
    "        X_train_1 = X_train[X_train.cluster == 1]\n",
    "\n",
    "        y_train_0 = y_train[X_train.cluster == 0]\n",
    "        y_train_1 = y_train[X_train.cluster == 1]\n",
    "        \n",
    "        self.model_0.fit(X_train_0, y_train_0)\n",
    "        self.model_1.fit(X_train_1, y_train_1)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred_kettle_0 = self.model_0.predict(X_test[X_test.cluster==0])\n",
    "        y_pred_kettle_1 = self.model_1.predict(X_test[X_test.cluster==1])\n",
    "        \n",
    "        df_0 = pd.DataFrame(y_pred_kettle_0, index=X_test[X_test.cluster == 0].index)\n",
    "        df_1 = pd.DataFrame(y_pred_kettle_1, index=X_test[X_test.cluster == 1].index)\n",
    "        y_pred_kettle = pd.concat([df_0, df_1])\n",
    "        y_pred_kettle.sort_index(inplace=True)\n",
    "        \n",
    "        y_pred_kettle = np.where(y_pred_kettle<0, 0, y_pred_kettle)\n",
    "        \n",
    "        return y_pred_kettle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0ffIqmBp2jH"
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "prep = kettle()\n",
    "X_train = prep.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "le79X29cp2gm"
   },
   "outputs": [],
   "source": [
    "# Drop na\n",
    "data = X_train.join(ytrain)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-5]\n",
    "y_train = data.iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW6Aqmv3p2eP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IApYbbXFp2cW",
    "outputId": "78e63c9a-9ed9-4199-92d7-1c10456b2aa5"
   },
   "outputs": [],
   "source": [
    "# Fitting of the model\n",
    "prep.fit(X_train, y_train.kettle)\n",
    "\n",
    "# Predict\n",
    "y_pred = prep.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "uWLJbJ8Zp2aE",
    "outputId": "8d6cdd47-02cc-4fd1-ee5a-9c0670ab08d3"
   },
   "outputs": [],
   "source": [
    "print(\"R2 of\", r2_score(y_test.kettle, y_pred))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test.kettle, y_pred)))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test.kettle, y_pred))*4.95/74.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PS7ekKyOrHpB"
   },
   "source": [
    "*Remark: the chosen hyperparameters are the results of a RandomizedSearchCV (sklearn)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoQBCVS_y8QI"
   },
   "source": [
    "## Washing_machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCwjoun9zcHf"
   },
   "source": [
    "**Pattern** \\\n",
    "Thanks to the Exploratory Data Analysis, we are able to notice that the pattern in the washing_machine consumption is often reflected on the pattern of the overall consumption. Therefore, we put in our training data the overall consumption over the past 30 minutes to caracterize the pattern. \n",
    "Thanks to this data we can say that, if the standard deviation is high and the mean is high, we are at the beginning of the peak: we should predict a high value for washing_machine.\n",
    "\n",
    "**Weather Information** \\\n",
    "We kept this information because it seems to have an influence on the washing_machine consumption. \n",
    "\n",
    "**Temporal Information** \\\n",
    "Week, weekday and hour of the day are taken into account. \n",
    "\n",
    "**Model** \\\n",
    "We use a boosting method: the Light GBM algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7M9ya8sy-de"
   },
   "outputs": [],
   "source": [
    "class washing_machine():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = 0\n",
    "        \n",
    "    def shift_data(self, data, lag=30):\n",
    "        series = [data.shift(i) for i in range(lag-1, -1, -1)]\n",
    "        new_df = pd.concat(series, axis=1)\n",
    "        name_col = []\n",
    "        \n",
    "        for i in range(lag, 0, -1):\n",
    "            var = \"consumption -\" + str(i)\n",
    "            name_col.append(var)\n",
    "        \n",
    "        new_df.columns = name_col\n",
    "        new_df[\"std\"] = new_df.std(axis=1)\n",
    "        new_df[\"mean\"] = new_df.mean(axis=1)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def transform(self, X_train):\n",
    "        \n",
    "        #Environment information\n",
    "        X_train = X_train.drop(columns=\"Unnamed: 9\")\n",
    "        X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "        X_train[\"week\"] = X_train[\"time_step\"].map(lambda x: x.week)\n",
    "        X_train[\"weekday\"] = X_train[\"time_step\"].map(lambda x: x.weekday)\n",
    "        X_train[\"hour\"] = X_train[\"time_step\"].map(lambda x: x.hour)\n",
    "        \n",
    "        #Missing values\n",
    "        X_train.fillna(method=\"ffill\", inplace=True) #Get the previous non null values\n",
    "        \n",
    "        #Supervised consumption\n",
    "        supervised_consumption = self.shift_data(X_train.consumption)\n",
    "        X_train = X_train.join(supervised_consumption)\n",
    "        \n",
    "        #One hot encoding\n",
    "        X_train_hour = pd.Categorical(X_train.hour, categories = [i for i in range(24)])\n",
    "        X_train_weekday = pd.Categorical(X_train.weekday, categories = [i for i in range(7)])\n",
    "        X_train_week = pd.Categorical(X_train.week, categories = [i for i in range(52)])\n",
    "        \n",
    "        X_train = X_train.join(pd.get_dummies(X_train_hour, prefix=\"hour\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_weekday, prefix=\"weekday\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_week, prefix=\"week\"))\n",
    "        \n",
    "        X_train.drop(columns=[\"time_step\", \"hour\", \"weekday\", \"week\"], inplace=True)\n",
    "        \n",
    "        return X_train\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        lgb_train = lgb.Dataset(X_train.values, y_train)\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'l2', 'l1'},\n",
    "            'num_leaves': 50,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 0}\n",
    "\n",
    "        self.model = lgb.train(params, lgb_train,\n",
    "                             num_boost_round=10000)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.model.predict(X_test, num_iteration=self.model.best_iteration)\n",
    "        y_pred = np.where(y_pred<0,0,y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bl4-GICqzD4U"
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "prep = washing_machine()\n",
    "X_train = prep.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwZGnOT2zIdG"
   },
   "outputs": [],
   "source": [
    "# Drop na\n",
    "data = X_train.join(ytrain)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-5]\n",
    "y_train = data.iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvdqiADmzLap"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBVdm87CzOd4"
   },
   "outputs": [],
   "source": [
    "# Fitting of the model\n",
    "prep.fit(X_train, y_train.washing_machine)\n",
    "\n",
    "# Predict\n",
    "y_pred = prep.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bHNBPi_NzT4O",
    "outputId": "0ea03407-aff4-446b-e15a-5fcd5c907862"
   },
   "outputs": [],
   "source": [
    "print(\"R2 of\", r2_score(y_test.washing_machine, y_pred))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test.washing_machine, y_pred)))\n",
    "print(\"Adjusted RMSE: \", np.sqrt(mean_squared_error(y_test.washing_machine, y_pred))*5.55/74.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmosXAen2QuR"
   },
   "source": [
    "## TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "no1KDlUv5kRD"
   },
   "source": [
    "For TV, we used Prophet, open source software released by Facebook’s Core Data Science team. This method outperformed all our other trials using boosting algorithms.\n",
    "\n",
    "Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality. It works best with time series that have strong seasonal effects and several seasons of historical data such as the consumption of TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVb_ltdw2Ux9"
   },
   "outputs": [],
   "source": [
    "class tv():\n",
    "    def __init__(self):\n",
    "        self.model = Prophet()\n",
    "    \n",
    "    def transform(self, X_train):\n",
    "        X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "        df = pd.DataFrame({\"ds\": X_train.time_step})\n",
    "        return df\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train['y'] = y_train\n",
    "        self.model.fit(X_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        pred = self.model.predict(X_test)\n",
    "        y_pred = np.where(pred.yhat <0, 0, pred.yhat)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAN2nZdx2ho-"
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "prep = tv()\n",
    "X_train = prep.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnQheF9F2hpE"
   },
   "outputs": [],
   "source": [
    "# Drop na\n",
    "data = X_train.join(ytrain)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-5]\n",
    "y_train = data.iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJKXFXPC2hpK"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MEHsbZSQ2hpQ",
    "outputId": "dc26df13-9e19-4bcd-e4c8-749b03aecc33"
   },
   "outputs": [],
   "source": [
    "# Fitting of the model\n",
    "prep.fit(X_train, y_train.TV)\n",
    "\n",
    "# Predict\n",
    "y_pred = prep.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NUPiYEOI2hpY",
    "outputId": "45435f93-20d9-4871-d7db-bb7171eb933b"
   },
   "outputs": [],
   "source": [
    "print(\"R2 of\", r2_score(y_test.TV, y_pred))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test.TV, y_pred)))\n",
    "print(\"Adjusted RMSE: \", np.sqrt(mean_squared_error(y_test.TV, y_pred))*14.57/74.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIc-8IlRG-ih"
   },
   "source": [
    "# Appendix\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMMnl506HCqJ"
   },
   "source": [
    "In this part of the report we introduice some of the ideas we had to solve the problem that were not as successful as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MRA5NDaHSQB"
   },
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_WnZ6uBHVhS"
   },
   "source": [
    "Since there was a lot of extreme values in the Washing_machine and Kettle target, we decided to try to apply a log transformation on the values: \n",
    "$$ y_{new} = log(1+y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shM6pg5XHWus"
   },
   "source": [
    "Let's see the results in the case of washing_machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "1lXCMcSqHPMe",
    "outputId": "77f8c273-9ae6-4867-8158-d999bd57f6ec"
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_train = Xtrain.copy()\n",
    "y_train = ytrain.copy()\n",
    "\n",
    "#Applying the log transformation\n",
    "y_new = np.log(1+y_train.washing_machine)\n",
    "\n",
    "#Plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(18,3))\n",
    "ax[0].hist(y_new)\n",
    "ax[0].set_title(\"Histogram of all the values\")\n",
    "ax[1].hist(y_new[y_new>=1])\n",
    "ax[1].set_title(\"Histogram for all values >= 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6h_HRWVHqsm"
   },
   "source": [
    "The data seems more normally distributed when values are higher than 1. This could help our model. \n",
    "\n",
    "This the data preparation and the model that will be used to predict $y_{new}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvn8wzmUHm7G"
   },
   "outputs": [],
   "source": [
    "class washing_machine():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = 0\n",
    "        \n",
    "    def shift_data(self, data, lag=30):\n",
    "        series = [data.shift(i) for i in range(lag-1, -1, -1)]\n",
    "        new_df = pd.concat(series, axis=1)\n",
    "        name_col = []\n",
    "        \n",
    "        for i in range(lag, 0, -1):\n",
    "            var = \"consumption -\" + str(i)\n",
    "            name_col.append(var)\n",
    "        \n",
    "        new_df.columns = name_col\n",
    "        new_df[\"std\"] = new_df.std(axis=1)\n",
    "        new_df[\"mean\"] = new_df.mean(axis=1)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def transform(self, X_train):\n",
    "        \n",
    "        #Environment information\n",
    "        X_train = X_train.drop(columns=\"Unnamed: 9\")\n",
    "        X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "        X_train[\"week\"] = X_train[\"time_step\"].map(lambda x: x.week)\n",
    "        X_train[\"weekday\"] = X_train[\"time_step\"].map(lambda x: x.weekday)\n",
    "        X_train[\"hour\"] = X_train[\"time_step\"].map(lambda x: x.hour)\n",
    "        \n",
    "        #Missing values\n",
    "        X_train.fillna(method=\"ffill\", inplace=True) #Get the previous non null values\n",
    "        \n",
    "        #Supervised consumption\n",
    "        supervised_consumption = self.shift_data(X_train.consumption)\n",
    "        X_train = X_train.join(supervised_consumption)\n",
    "        \n",
    "        #One hot encoding\n",
    "        X_train_hour = pd.Categorical(X_train.hour, categories = [i for i in range(24)])\n",
    "        X_train_weekday = pd.Categorical(X_train.weekday, categories = [i for i in range(7)])\n",
    "        X_train_week = pd.Categorical(X_train.week, categories = [i for i in range(52)])\n",
    "        \n",
    "        X_train = X_train.join(pd.get_dummies(X_train_hour, prefix=\"hour\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_weekday, prefix=\"weekday\"))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train_week, prefix=\"week\"))\n",
    "        \n",
    "        X_train.drop(columns=[\"time_step\", \"hour\", \"weekday\", \"week\"], inplace=True)\n",
    "        \n",
    "        return X_train\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        lgb_train = lgb.Dataset(X_train.values, y_train)\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'l2', 'l1'},\n",
    "            'num_leaves': 50,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 0}\n",
    "\n",
    "        self.model = lgb.train(params, lgb_train,\n",
    "                             num_boost_round=5000)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.model.predict(X_test, num_iteration=self.model.best_iteration)\n",
    "        y_pred = np.where(y_pred<0,0,y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "c6VD6ln3Hs07",
    "outputId": "6bc126d1-947c-4925-8a72-f53e4c046dca"
   },
   "outputs": [],
   "source": [
    "wash = washing_machine()\n",
    "X_train = wash.transform(X_train)\n",
    "\n",
    "#Drop na\n",
    "data = X_train.join(y_new)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-1]\n",
    "y_train = data.iloc[:,-1:]\n",
    "\n",
    "#Train-test splot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, \n",
    "                                                    shuffle=False, random_state=42)\n",
    "\n",
    "#Fit and predict\n",
    "wash.fit(X_train, y_train)\n",
    "y_pred = np.exp(wash.predict(X_test)) - 1\n",
    "\n",
    "print(\"R2 of \", r2_score(np.exp(y_test)-1, y_pred))\n",
    "print(\"RMSE of \", np.sqrt(mean_squared_error(np.exp(y_test)-1, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzLKN_nRH_FG"
   },
   "source": [
    "On the testing set the scores are quite bad, so we decided to drop that idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViEZ13lEIAbe"
   },
   "source": [
    "## Model Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBo9Znj3IF4l"
   },
   "source": [
    "We also tried to do some model stacking for Kettle. The models used are the following: \\\\\n",
    "- Random Forest \\\\\n",
    "- XGBoost \\\\\n",
    "- LGBM \n",
    "\n",
    "Then, we took them as input of a Neural Network with the following structure: \\\\\n",
    "- First layer: 12 neurons \\\\\n",
    "- Second layer: 6 neurons \\\\\n",
    "- Third layer: 3 neurons \\\\\n",
    "- Output layer (1 neuron)\n",
    "\n",
    "To fit the model, we split our training data into 4 sets of equal size. \\\\\n",
    "- The 1st set was used to train the Random Forest model \\\\\n",
    "- The 2nd set was used to train the XGBoost model \\\\\n",
    "- The 3rd set was used to train the LGBM model \\\\\n",
    "- The 4th set was used to train the neural network \n",
    "\n",
    "Let's see the application of that in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRY0D_eXINIS"
   },
   "outputs": [],
   "source": [
    "def shift_data(data, lag=20):\n",
    "    series = [data.shift(i) for i in range(lag-1, -1, -1)]\n",
    "    new_df = pd.concat(series, axis=1)\n",
    "    name_col = []\n",
    "\n",
    "    for i in range(lag, 0, -1):\n",
    "        var = \"consumption -\" + str(i)\n",
    "        name_col.append(var)\n",
    "\n",
    "    new_df.columns = name_col\n",
    "    new_df[\"std\"] = new_df.std(axis=1)\n",
    "    new_df[\"mean\"] = new_df.mean(axis=1)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def transform(X_train):\n",
    "    X_train = X_train[[\"time_step\", \"consumption\"]]\n",
    "    X_train.time_step = pd.to_datetime(X_train.time_step)\n",
    "\n",
    "    X_train[\"hour\"] = X_train[\"time_step\"].map(lambda x: x.hour)\n",
    "    X_train[\"weekday\"] = X_train[\"time_step\"].map(lambda x: x.dayofweek)\n",
    "    X_train[\"week\"] = X_train[\"time_step\"].map(lambda x: x.week)\n",
    "\n",
    "    #Missing values\n",
    "    X_train.fillna(method=\"ffill\", inplace=True) #Get the previous non null values\n",
    "\n",
    "    #Supervised consumption\n",
    "    supervised_consumption = shift_data(X_train.consumption)\n",
    "\n",
    "    #One hot encoding\n",
    "    X_train_hour = pd.Categorical(X_train.hour, categories = [i for i in range(24)])\n",
    "    X_train_weekday = pd.Categorical(X_train.weekday, categories = [i for i in range(7)])\n",
    "    X_train_week = pd.Categorical(X_train.week, categories = [i for i in range(52)])\n",
    "\n",
    "    X_train = X_train.join(pd.get_dummies(X_train_hour, prefix=\"hour\"))\n",
    "    X_train = X_train.join(pd.get_dummies(X_train_weekday, prefix=\"weekday\"))\n",
    "    X_train = X_train.join(pd.get_dummies(X_train_week, prefix=\"week\"))\n",
    "\n",
    "    X_train.drop(columns=[\"time_step\",\"hour\", \"weekday\", \"week\"], inplace=True)\n",
    "\n",
    "    return X_train.join(supervised_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9hiaO69JINl-",
    "outputId": "c2404474-bcf5-4f51-b4c0-cdb3766de42a"
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_train = Xtrain.copy()\n",
    "y_train = ytrain.copy()\n",
    "X_train = transform(X_train)\n",
    "\n",
    "#Drop na for training\n",
    "data = X_train.join(y_train)\n",
    "data = data.dropna()\n",
    "X_train = data.iloc[:,:-5]\n",
    "y_train = data.iloc[:,-5:]\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Split the data into 4 sets\n",
    "index = X_train.index\n",
    "new_index = np.random.permutation(index)[:-5]\n",
    "size = int(np.round(len(new_index)/4))\n",
    "a = [new_index[i:i + size] for i in range(0, len(new_index), size)]\n",
    "\n",
    "X_train0 = X_train.iloc[a[0],:]\n",
    "X_train1 = X_train.iloc[a[1],:]\n",
    "X_train2 = X_train.iloc[a[2],:]\n",
    "X_train3 = X_train.iloc[a[3],:]\n",
    "\n",
    "y_train0 = y_train.iloc[a[0],:]\n",
    "y_train1 = y_train.iloc[a[1],:]\n",
    "y_train2 = y_train.iloc[a[2],:]\n",
    "y_train3 = y_train.iloc[a[3],:]\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model0 = RandomForestRegressor()\n",
    "model0.fit(X_train0, y_train0.kettle)\n",
    "feature0 = model0.predict(X_train3)\n",
    "\n",
    "# XGB\n",
    "model1 = xgb.XGBRegressor()\n",
    "model1.fit(X_train1, y_train1.kettle)\n",
    "feature1 = model1.predict(X_train3)\n",
    "\n",
    "#LGBM\n",
    "model2 = lgb.LGBMRegressor()\n",
    "model2.fit(X_train2, y_train2.kettle)\n",
    "feature2 = model2.predict(X_train3)\n",
    "\n",
    "# Input for the Neural Network\n",
    "new_df = pd.DataFrame({\"RF\": feature0, \"XGB\": feature1, \"LGB\": feature2})\n",
    "new_df = np.where(new_df<0, 0, new_df)\n",
    "\n",
    "# Neural Network structure\n",
    "from tensorflow.python.keras.layers import Dense, LSTM\n",
    "from tensorflow.python.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim= new_df.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(6, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "eS3tL_SnIZCD",
    "outputId": "f4adceb5-9e73-4ce3-bfc1-22a14cec5f66"
   },
   "outputs": [],
   "source": [
    "model.fit(new_df, y_train3.kettle, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1rFk4RaWIf0p",
    "outputId": "381252de-e268-44f5-9c13-7fc5ca9a7612"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(new_df)\n",
    "y_pred = np.where(y_pred<0, 0, y_pred)\n",
    "\n",
    "print(\"R2 of \", r2_score(y_train3.kettle, y_pred))\n",
    "print(\"RMSE of \", np.sqrt(mean_squared_error(y_train3.kettle, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpnFkK2EIjW0"
   },
   "source": [
    "The results are quite good but it was not our best result on the online platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvSMyWKHImOR"
   },
   "source": [
    "## Other ideas for Kettle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsV8fe4iKGUl"
   },
   "source": [
    "We had the following ideas for Kettle:\n",
    "\n",
    "**Feature Label**\\\n",
    "Adding a feature that labels 1 the rows where kettle is different than zero and 0 the rows where kettle is null. We created this feature by using an XGBoost Classifier. As the dataset is very unbalanced (there is a majority of rows labeled 0), we used the parameter scale_pos_weight=3.7 to Control the balance of positive and negative weights. \n",
    "\n",
    "**Additionnal features** \\\n",
    "Adding the temporal features half_hour, season, timeofday, daylight inversed.\n",
    "\n",
    "**Validation set**\\\n",
    "We noticed that the test set is indexed by the months January to June 2014. In the training set we only have data from March 17 to the end of December 2013. So to take this into account, we chose a validation set with a month that is not present in the training set (month 7) and 4 four weeks of months that are present in the training set (week 14 of month 3, week 15 of month 4, week 43 and 44 of October).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qtcwylDPIikm"
   },
   "outputs": [],
   "source": [
    "class kettle():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = 0\n",
    "        \n",
    "    def label(self,x):\n",
    "        if x == 0 :\n",
    "            return 0\n",
    "        else :\n",
    "            return 1\n",
    "        \n",
    "    def prep_kettle(self,X_train,y_train):\n",
    "        data = X_train\n",
    "        data_month = data['month'].copy()\n",
    "        data_week = data['week'].copy()\n",
    "\n",
    "        #past consumption\n",
    "        for i in range(10, -1, -1):\n",
    "            data['conso-'+str(i)] = data.consumption.shift(i)\n",
    "\n",
    "        #one hot encoding\n",
    "        data = data.join(pd.get_dummies(data.half_hour, prefix=\"half_hour\"))\n",
    "        data = data.join(pd.get_dummies(data.weekday, prefix=\"weekday\"))\n",
    "        data = data.join(pd.get_dummies(data.week, prefix=\"week\"))\n",
    "        data = data.join(pd.get_dummies(data.season, prefix=\"season\"))\n",
    "        data = data.join(pd.get_dummies(data.timeofday, prefix=\"timeofday\"))\n",
    "\n",
    "        data = data.drop(columns=[\"half_hour\",\"weekday\",\"week\",\"season\",\"timeofday\",\"hour\",\"month\",'conso-0'])\n",
    "        data = data.drop(columns=[\"visibility\",\"humidity\",\"windchill\",\"wind\",\"pressure\"])\n",
    "\n",
    "        return data, data_month, data_week\n",
    "    \n",
    "    def XGB(self,X_train,y_train,k):\n",
    "        data_kettle, data_month, data_week = self.prep_kettle(X_train,y_train)\n",
    "        data_kettle = pd.concat([data_kettle,y_train.kettle], axis=1)\n",
    "        data_kettle['label'] = data_kettle.kettle.apply(lambda x: self.label(x))\n",
    "\n",
    "        #Train-test split\n",
    "        test_set = pd.concat([data_kettle[data_month == 7],\n",
    "                          data_kettle[data_week == {14,15,43,44}]])\n",
    "\n",
    "        train_set = data_kettle.loc[np.delete(data_kettle.index, test_set.index)]\n",
    "\n",
    "        X_train = train_set.drop(['kettle','label'], axis = 1)\n",
    "        y_train = train_set.label\n",
    "\n",
    "        X_test = test_set.drop(['kettle','label'], axis = 1)\n",
    "        y_test = test_set.label\n",
    "\n",
    "        #Model\n",
    "        model = XGBClassifier(scale_pos_weight=k,random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def transform(self, X_train,y_train):       \n",
    "        X_train = X_prep(X_train)\n",
    "        y_train = y_prep(y_train)\n",
    "        data_kettle, data_month, data_week = self.prep_kettle(X_train,y_train)\n",
    "        y_test_label = self.XGB(X_train,y_train,3.7)\n",
    "\n",
    "        #Splitting train and test sets\n",
    "        X_test = pd.concat([data_kettle[data_month == 7],\n",
    "                            data_kettle[data_week == {14,15,43,44}]])\n",
    "        X_train = data_kettle.loc[np.delete(data_kettle.index, X_test.index)]\n",
    "        \n",
    "        y_test = pd.concat([y_train.kettle[data_month == 7],\n",
    "                            y_train.kettle[data_week == {14,15,43,44}]])\n",
    "        y_train = y_train.kettle.loc[np.delete(y_train.kettle.index, y_test.index)]\n",
    "        \n",
    "        #Adding the feature label\n",
    "        X_train['label'] = y_train.apply(lambda x: self.label(x))\n",
    "        X_train = X_train.join(pd.get_dummies(X_train.label, prefix=\"label\"))\n",
    "        X_train = X_train.drop('label', axis = 1)\n",
    "        \n",
    "        X_test['label'] = y_test_label\n",
    "        X_test = X_test.join(pd.get_dummies(X_test.label, prefix=\"label\"))\n",
    "        X_test = X_test.drop('label', axis = 1)\n",
    "        \n",
    "        #Ensuring the same features for X_train and X_test\n",
    "        col_dif = set(X_train.columns) - set(X_test.columns)\n",
    "        for i in col_dif:\n",
    "            X_test[i] = 0\n",
    "\n",
    "        col_dif = set(X_test.columns) - set(X_train.columns)\n",
    "        for i in col_dif:\n",
    "            X_train[i] = 0\n",
    "            \n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train,X_test):\n",
    "        model_kettle = xgb.XGBRegressor(random_state=42) \n",
    "        model_kettle.fit(X_train.values, y_train.values)\n",
    "        y_pred_kettle = model_kettle.predict(X_test.values)\n",
    "        y_pred_kettle[y_pred_kettle<0] = 0\n",
    "        return y_pred_kettle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AgvYsD4aOeJI",
    "outputId": "50f623cf-7d0e-4905-ee73-e62b38372e00"
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_train = Xtrain.copy()\n",
    "y_train = ytrain.copy()\n",
    "\n",
    "#Transform\n",
    "kettle = kettle()\n",
    "X_train, y_train, X_test, y_test = kettle.transform(X_train,y_train)\n",
    "\n",
    "#Model\n",
    "y_pred = kettle.fit_predict(X_train, y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mZpvqrB8Os8u",
    "outputId": "d5e7557e-b60e-42e6-c470-076086a92adc"
   },
   "outputs": [],
   "source": [
    "print(\"R2 of\", r2_score(y_test, y_pred))\n",
    "print(\"RMSE of\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"Adjusted RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred))*4.95/74.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxImlFHYT5tq"
   },
   "source": [
    "This is a satisfying RMSE for kettle, however when we submitted on the platform using this method, we obtained a score worse than before and decided to drop this idea."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kYU3eBv6r8j8",
    "Bhrgltsyp-S4"
   ],
   "name": "Projet Electricity 01-04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
